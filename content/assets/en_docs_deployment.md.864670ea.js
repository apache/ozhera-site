import{_ as o,a as t,b as r,c as i,d as n,e as s,f as l,g as c,h,i as p,j as d,k as u,l as m,m as f,n as g,o as y,p as b,q as z,r as _,s as q,t as v,u as k,v as x,w as P,x as w,y as O,z as T,A as H,B as C,C as j,D as I,E as S,F as E}from"./chunks/ozhera-log-manager.039ef01c.js";import{_ as R,o as N,h as M,Q as a,p as e,e as A}from"./chunks/framework.d1267443.js";const Z=JSON.parse('{"title":"OzHera Deployment Documentation","description":"","frontmatter":{"outline":"deep"},"headers":[],"relativePath":"en/docs/deployment.md","filePath":"en/docs/deployment.md"}'),D={name:"en/docs/deployment.md"},Q=a('<h1 id="ozhera-deployment-documentation" tabindex="-1">OzHera Deployment Documentation <a class="header-anchor" href="#ozhera-deployment-documentation" aria-label="Permalink to &quot;OzHera Deployment Documentation&quot;">​</a></h1><h2 id="_1-deployment-instructions" tabindex="-1">1. Deployment Instructions <a class="header-anchor" href="#_1-deployment-instructions" aria-label="Permalink to &quot;1. Deployment Instructions&quot;">​</a></h2><p>The purpose of the OzHera operator is to launch an OzHera platform in a specified namespace in the k8s cluster. This documentation is suitable for R&amp;D/operations staff with basic k8s knowledge (PV, PVC, Service, Pod, Deployment, DaemonSet, etc.).</p><p>OzHera is an enterprise-level observability platform with a very high complexity during deployment. Please read the following deployment documentation and related <a href="https://mp.weixin.qq.com/s?__biz=MzkwMjQzMzMxMg==&amp;mid=2247483720&amp;idx=1&amp;sn=c38fca2d3e82de43ce22acad73a1be21&amp;chksm=c0a4de07f7d35711c5cba634c3833708db19fcc9303a50b77f8c1601831cac8e9520e3f32ff5&amp;token=1000658198&amp;lang=zh_CN#rd" target="_blank" rel="noreferrer">operator introduction video</a> carefully before deployment.</p><h2 id="_2-deployment-steps" tabindex="-1">2. Deployment Steps <a class="header-anchor" href="#_2-deployment-steps" aria-label="Permalink to &quot;2. Deployment Steps&quot;">​</a></h2><p>ozhera-all/ozhera-operator/ozhera-operator-server/src/main/resources/operator/</p><h3 id="_2-1-create-a-separate-namespace-and-account" tabindex="-1">2.1 Create a Separate Namespace and Account <a class="header-anchor" href="#_2-1-create-a-separate-namespace-and-account" aria-label="Permalink to &quot;2.1 Create a Separate Namespace and Account&quot;">​</a></h3><ul><li><p>Execute the command to generate the auth yaml (by default, it will create a space: ozhera-namespace and account: admin-mone):</p><p><code>kubectl apply -f ozhera_operator_auth.yaml</code></p></li></ul><h3 id="_2-2-create-ozhera-crd" tabindex="-1">2.2 Create ozhera CRD <a class="header-anchor" href="#_2-2-create-ozhera-crd" aria-label="Permalink to &quot;2.2 Create ozhera CRD&quot;">​</a></h3><ul><li><p>Execute the command to generate the crd yaml:</p><p><code>kubectl apply -f ozhera_operator_crd.yaml</code></p></li></ul><h3 id="_2-3-deploy-operator" tabindex="-1">2.3 Deploy operator <a class="header-anchor" href="#_2-3-deploy-operator" aria-label="Permalink to &quot;2.3 Deploy operator&quot;">​</a></h3><ul><li><p>Execute the command to deploy the operator:</p><p><code>kubectl apply -f ozhera_operator_deployment.yaml</code></p></li><li><p>Ensure that the deployed operator service port 7001 is externally accessible. The deployment of ozhera requires operations on the external page provided by the operator. In the default example, the LoadBalancer method is used to expose the externally accessible ip and port. If other methods are needed, modify the yaml yourself.</p></li></ul><p><img src="'+o+'" alt="ozhera-operator-deployment.png"></p><h3 id="_2-4-operator-page-operations" tabindex="-1">2.4 Operator Page Operations <a class="header-anchor" href="#_2-4-operator-page-operations" aria-label="Permalink to &quot;2.4 Operator Page Operations&quot;">​</a></h3><h4 id="_2-4-1-access-the-operator-page" tabindex="-1">2.4.1 Access the operator page <a class="header-anchor" href="#_2-4-1-access-the-operator-page" aria-label="Permalink to &quot;2.4.1 Access the operator page&quot;">​</a></h4><p>If you use the LoadBalancer method in step 2.3, first find the external IP of the &quot;ozhera-op-nginx&quot; service. Execute the command:</p><p><code>kubectl get service -n=ozhera-namespace</code></p><p>Find the EXTERNAL-IP corresponding to ozhera-op-nginx. The default access address is: <code>http://EXTERNAL-IP:80/</code></p><p>You will see the following interface:</p><p><img src="'+t+'" alt="operator-home-page.jpg"></p><h4 id="_2-4-2-operator-metadata-entry" tabindex="-1">2.4.2 Operator metadata entry <a class="header-anchor" href="#_2-4-2-operator-metadata-entry" aria-label="Permalink to &quot;2.4.2 Operator metadata entry&quot;">​</a></h4><ul><li><p>name: ozhera-bootstrap</p><p>k8s custom resource name, keep the default value unchanged.</p></li><li><p>Namespace: ozhera-namespace</p><p>ozhera&#39;s independent space, it is recommended to keep ozhera-namespace unchanged. If changes are required, note the global change of the yaml.</p></li></ul><h4 id="_2-4-3-confirm-k8s-access-method" tabindex="-1">2.4.3 Confirm k8s access method <a class="header-anchor" href="#_2-4-3-confirm-k8s-access-method" aria-label="Permalink to &quot;2.4.3 Confirm k8s access method&quot;">​</a></h4><p>This step is to generate the access ip:port of the external page required in the ozhera platform. Currently, only the k8s LoadBalancer and NodePort methods are supported. By default, the LB mode will be tried first. If not supported, select NodePort (if the IP of NodePort is not open for external access, you need to set up a proxy separately, it is recommended that the cluster turn on LB).</p><p><img src="'+r+'" alt="operator-interview1.jpg"><img src="'+i+'" alt="operator-interview2.jpg"></p>',25),B=e("p",null,[A("Remember ozhera.homepage.url, after the ozhera cluster is built, the default access address is: "),e("a",{href:"http://$","ozhera.homepage.url":"",target:"_blank",rel:"noreferrer"},"http://$")],-1),F=a('<h4 id="_2-4-4-cluster-configuration" tabindex="-1">2.4.4 Cluster Configuration <a class="header-anchor" href="#_2-4-4-cluster-configuration" aria-label="Permalink to &quot;2.4.4 Cluster Configuration&quot;">​</a></h4><p><strong>Please do not modify k8s-serviceType</strong></p><p><img src="'+n+'" alt="operator-service-type.jpg"></p><h5 id="ozhera-mysql" tabindex="-1">OzHera-mysql <a class="header-anchor" href="#ozhera-mysql" aria-label="Permalink to &quot;OzHera-mysql&quot;">​</a></h5><p>The purpose is to select a usable mysql database for ozhera.</p><ul><li><p>If you need k8s to automatically set up a database:</p><p>Turn on the &quot;Create resources based on yaml&quot; button. The default yaml will create a pv for mysql data storage. If you use the default yaml, be sure to:</p><ol><li>Create a directory /opt/ozhera_pv/ozhera_mysql on the host machine node in advance (the directory can be changed, and this yaml is synchronized to modify);</li><li>Find the name of the node where the directory was created (you can execute kubectl get node to confirm) and replace the value of cn-bxxx52 here;</li><li>Ensure that the connection information is consistent with the information in the yaml, no modification is required by default.</li></ol></li></ul><p><img src="'+s+'" alt="ozhera-mysql.jpg"></p><ul><li>If you already have a database and don&#39;t need k8s to create it: <ol><li>Turn off the &quot;Create resources based on yaml&quot; button;</li><li>Fill in the correct existing database url, username, and password;</li><li>By default, the operator will automatically modify the database to create the ozhera database and table. <strong>If the account you entered does not have permission to create a library or table, you need to manually create the ozhera database and table in the target library in advance.</strong> The create table statement is in the operator source code ozhera-all/ozhera-operator/ozheraoperator-server/src/main/resources/ozhera_init/mysql/sql directory.</li></ol></li></ul><p><img src="'+l+'" alt="ozhera-mysql2.jpg"></p><h5 id="ozhera-es" tabindex="-1">OzHera-es <a class="header-anchor" href="#ozhera-es" aria-label="Permalink to &quot;OzHera-es&quot;">​</a></h5><p>The goal is to select an ES cluster available to OzHera and create the index template required by OzHera in ES.</p><ul><li><p>If you need k8s to automatically set up an ES:</p><p>You need to turn on the &quot;Create resources based on yaml&quot; button. The default yaml-created ES has no account or password. If you need to set up an account or password, you need to:</p><ol><li>Modify xpack.security.enabled in the left yaml to true;</li><li>Modify the values of ozhera.es.username and ozhera.es.password on the right &quot;Connection Information&quot;, generally, we will use the elastic account, and the password needs to be set after the ES service is started;</li><li>After starting ES, log in to the pod where ES is located, enter the /usr/share/elasticsearch/bin directory, execute the elasticsearchsetup-passwords interactive command, set the default account password for ES, note that the password set here must be consistent with the ozhera.es.password value on the page.</li></ol></li></ul><p><img src="'+c+'" alt="ozhera-es.jpg"></p><ul><li>If you already have ES, you don&#39;t need k8s to create it: <ol><li>Turn off the &quot;Create resources based on yaml&quot; button;</li><li>Fill in the correct url, account, and password of the existing ES cluster;</li><li>By default, the operator will automatically create the index template. <strong>If the account you entered does not have permission to create an index template, you need to manually create the index template required by OzHera in advance</strong>. OzHera&#39;s index template is stored in the operator source code run.mone.ozhera.operator.common.ESIndexConst in json format.</li></ol></li></ul><p><img src="'+h+'" alt="ozhera-es2.jpg"></p><h5 id="ozhera-rocketmq" tabindex="-1">OzHera-rocketMQ <a class="header-anchor" href="#ozhera-rocketmq" aria-label="Permalink to &quot;OzHera-rocketMQ&quot;">​</a></h5><p>The purpose is to select a RocketMQ available for ozhera.</p><ul><li>If you need k8s to automatically set up a RocketMQ: <ol><li>You need to turn on the &quot;Create resources based on yaml&quot; button;</li><li>The RocketMQ created by the default yaml has no accessKey or secretKey. If you need to set up accessKey or secretKey, you need to modify the values of ozhera.rocketmq.ak and ozhera.rocketmq.sk on the right &quot;Connection Information&quot;;</li><li>If you need to change the service of the RocketMQ broker, you need to replace the service in the yaml and the &quot;brokerAddr&quot; member variable value of the run.mone.ozhera.operator.service.RocketMQSerivce class in the ozoperator code.</li></ol></li></ul><p><img src="'+p+'" alt="ozhera-rocketmq.jpg"></p><ul><li>If you already have RocketMQ, you don&#39;t need k8s to set it up: <ol><li>Turn off the &quot;Create resources based on yaml&quot; button;</li><li>Fill in the correct url, accessKey, and secretKey of the existing RocketMQ cluster;</li><li>By default, the operator will automatically create the topics required by OzHera. <strong>If the url, ak, and sk you entered do not have permission to create topics, or if the existing RocketMQ cluster does not allow topic creation through the API, you need to manually create the required topics in advance</strong>. The topics required by OzHera are stored in the &quot;topics&quot; member variable of the run.mone.ozhera.operator.service.RocketMQSerivce class.</li></ol></li></ul><p><img src="'+d+'" alt="ozhera-rocketmq2.jpg"></p><h5 id="ozhera-redis" tabindex="-1">OzHera-redis <a class="header-anchor" href="#ozhera-redis" aria-label="Permalink to &quot;OzHera-redis&quot;">​</a></h5><p>The goal is to select a Redis available for ozhera.</p><ul><li><p>If you need k8s to automatically set up a Redis:</p><p>You need to turn on the &quot;Create resources based on yaml&quot; button. The default yaml-created redis has no password. If you need to set a password, you need to modify the value of ozhera.redis.password on the right to be consistent with the password set for redis.</p></li></ul><p><img src="'+u+'" alt="ozhera-redis.jpg"></p><ul><li>If you already have Redis, you don&#39;t need k8s to set it up: <ol><li>Turn off the &quot;Create resources based on yaml&quot; button;</li><li>Fill in the correct URL and password of the existing Redis cluster.</li></ol></li></ul><p><img src="'+m+'" alt="ozhera-redis2.jpg"></p><h5 id="ozhera-nacos" tabindex="-1">OzHera-Nacos <a class="header-anchor" href="#ozhera-nacos" aria-label="Permalink to &quot;OzHera-Nacos&quot;">​</a></h5><p>It is the configuration and registration center inside the ozhera cluster. This cluster is recommended to use the yaml creation method. If the business needs to provide Nacos by itself, please provide the Nacos version 1.x first.</p><ul><li><p>If you need k8s to automatically set up a Nacos:</p><p>You need to turn on the &quot;Create resources based on yaml&quot; button. Note that the image address, resource size configuration in the yaml, and the connection information on the right are consistent with the yaml.</p></li></ul><p><img src="'+f+'" alt="ozhera-nacos.jpg"></p><ul><li><p>If you already have Nacos, you don&#39;t need k8s to create it:</p><p>You need to turn off the &quot;Create resources based on yaml&quot; button and fill in the correct nacos connection information.</p></li></ul><p><img src="'+g+'" alt="ozhera-nacos2.jpg"></p><ul><li><p>Nacos configuration:</p><p>The operator will default to initializing the listed configurations as nacos configurations. If the provided nacos is not created based on yaml, please confirm that the connection information has the right to call the config creation interface, otherwise, you need to manually create it in the target nacos in advance.</p></li></ul><p><img src="'+y+'" alt="ozhera-nacos3.jpg"></p><h5 id="ozhera-prometheus" tabindex="-1">OzHera-Prometheus <a class="header-anchor" href="#ozhera-prometheus" aria-label="Permalink to &quot;OzHera-Prometheus&quot;">​</a></h5><p>The goal is to select a prometheus available for ozhera.</p><p>If you use the default yaml, be sure to:</p><ul><li>Create a directory /home/work/prometheus_ozhera_namespace_pv on the host machine node in advance;</li><li>Find the name of the node where the directory was created (you can execute kubectl get node to confirm) and replace the cn- xxx here.</li></ul><p><img src="'+b+'" alt="ozhera-prometheus.jpg"></p><h5 id="ozhera-alertmanager" tabindex="-1">OzHera-Alertmanager <a class="header-anchor" href="#ozhera-alertmanager" aria-label="Permalink to &quot;OzHera-Alertmanager&quot;">​</a></h5><p>The goal is to select an alertmanager available for ozhera.</p><p>If you use the default yaml, be sure to:</p><ul><li>Create a directory /home/work/alertmanager_ozhera_namespace_pv on the host machine node in advance;</li><li>Find the name of the node where the directory was created (you can execute kubectl get node to confirm) and replace the cn- here.</li></ul><p><img src="'+z+'" alt="ozhera-alertmanager.jpg"></p><h5 id="ozhera-grafana" tabindex="-1">OzHera-Grafana <a class="header-anchor" href="#ozhera-grafana" aria-label="Permalink to &quot;OzHera-Grafana&quot;">​</a></h5><p>The goal is to select a grafana available for ozhera.</p><p>If you use the default yaml, be sure to:</p><ul><li>Create a directory /home/work/grafana_ozhera_namespace_pv on the host machine node in advance;</li><li>Find the name of the node where the directory was created (you can execute kubectl get node to confirm) and replace the cn- beijingxxx here;</li><li>The content of host, user, port, password, etc. configured in OzHera-mysql needs to be overridden in the corresponding db configuration of ozhera-grafana.</li></ul><p><img src="'+_+'" alt="ozhera-grafana.jpg"></p><p><img src="'+q+'" alt="ozhera-grafana2.jpg"></p><h5 id="ozhera-node-exporter" tabindex="-1">OzHera-Node-exporter <a class="header-anchor" href="#ozhera-node-exporter" aria-label="Permalink to &quot;OzHera-Node-exporter&quot;">​</a></h5><p>The goal is to select a node-exporter available for ozhera.</p><p>If you use the default yaml, be sure to:</p><ul><li>Find a usable port on the host machine in advance and fill it in the hostPort shown below. The default is 9101. After modification, update the content of mione.k8s.node.port in the connection information on the right.</li></ul><p><img src="'+v+'" alt="ozhera-node-exporter.jpg"></p><h5 id="ozhera-cadvisor" tabindex="-1">OzHera-Cadvisor <a class="header-anchor" href="#ozhera-cadvisor" aria-label="Permalink to &quot;OzHera-Cadvisor&quot;">​</a></h5><p>The goal is to select a cadvisor available for ozhera.</p><p>If you use the default yaml, be sure to:</p><ul><li>Find a usable port on the host machine in advance and fill it in the hostPort shown below. The default is 5195. After modification, update the content of mione.k8s.container.port in the connection information on the right.</li></ul><p><img src="'+k+'" alt="ozhera-cadvisor.jpg"></p><h5 id="ozhera-trace-etl-es" tabindex="-1">OzHera-trace-etl-es <a class="header-anchor" href="#ozhera-trace-etl-es" aria-label="Permalink to &quot;OzHera-trace-etl-es&quot;">​</a></h5><p>Please note:</p><ul><li>This service is a StatefulSet type service;</li><li>Create a directory /home/work/rocksdb on the host machine node in advance (you can change the directory and synchronize to modify this yaml);</li><li>Find the name of the node where the directory was created (you can execute kubectl get node to confirm) and replace the value under nodeSelectorTerms here;</li><li>The number of replicas of the service pod should be consistent with the queue size of RocketMQ based on the trace traffic.</li></ul><p><img src="'+x+'" alt="ozhera-trace-etl-es.jpg"></p><h5 id="ozhera-trace-etl-manager" tabindex="-1">OzHera-trace-etl-manager <a class="header-anchor" href="#ozhera-trace-etl-manager" aria-label="Permalink to &quot;OzHera-trace-etl-manager&quot;">​</a></h5><p>Please note:</p><ul><li>The number of pod replicas and pod resource limits should be adjusted based on the amount of traffic.</li></ul><p><img src="'+P+'" alt="ozhera-trace-etl-manager.jpg"></p><h5 id="ozhera-trace-etl-server" tabindex="-1">OzHera-trace-etl-server <a class="header-anchor" href="#ozhera-trace-etl-server" aria-label="Permalink to &quot;OzHera-trace-etl-server&quot;">​</a></h5><p>Please note:</p><ul><li>The number of pod replicas and pod resource limits should be adjusted based on the amount of traffic;</li><li>The number of replicas of the service pod should be consistent with the queue size of RocketMQ.</li></ul><p><img src="'+w+'" alt="ozhera-trace-etl-server.jpg"></p><h5 id="ozhera-monitor" tabindex="-1">OzHera-monitor <a class="header-anchor" href="#ozhera-monitor" aria-label="Permalink to &quot;OzHera-monitor&quot;">​</a></h5><p>ozhera-monitor is the backend service of the hera monitoring homepage, metric monitoring, and alert configuration. It is recommended to use the deployment method provided by the operator based on the creation of resources in yaml. Of course, you can also deploy the ozhera-monitor service yourself (turn off the switch for creating resources based on yaml), and adjust the corresponding parameters for the frontend connection when deploying the service yourself: such as IP address, port number, etc.</p><p>Please note:</p><ul><li><p>MySQL</p><p>Configure the MySQL database in nacos and initialize the database table under the corresponding database name according to the sql file in ozhera-all/ozhera-monitor.</p></li><li><p>RocketMQ</p><p>Create the corresponding mq topic and tag on the corresponding RocketMQ server according to the configuration on nacos.</p></li><li><p>ES</p><p>Create the corresponding ES index on the corresponding ES server according to the configuration on nacos.</p></li><li><p>When using the operator to automatically create resources, you can adjust the number of replicas, replicas, according to your actual traffic situation. There is one replica in the instance; similarly, you can adjust the relevant resources of k8s, such as cpu, memory, in the yaml file of the operator.</p></li></ul><p><img src="'+O+'" alt="ozhera-monitor.jpg"></p><h5 id="ozhera-fe" tabindex="-1">OzHera-fe <a class="header-anchor" href="#ozhera-fe" aria-label="Permalink to &quot;OzHera-fe&quot;">​</a></h5><p>ozhera-fe is responsible for building the yaml of the ozhera front page.</p><p>Please note:</p><ul><li>The number of pod replicas and pod resource limits should be adjusted based on the amount of traffic.</li></ul><p><img src="'+T+'" alt="ozhera-fe.jpg"></p><h5 id="ozhera-tpc-login" tabindex="-1">OzHera-tpc-login <a class="header-anchor" href="#ozhera-tpc-login" aria-label="Permalink to &quot;OzHera-tpc-login&quot;">​</a></h5><p>ozhera-tpc-login is responsible for building the yaml of the tpclogin login service backend.</p><p>Please note:</p><ul><li>The number of pod replicas and pod resource limits should be adjusted based on the amount of traffic;</li><li>The configuration information for the service startup is in the nacos configuration above.</li></ul><p><img src="'+H+'" alt="ozhera-tpc.jpg"></p><h5 id="ozhera-tpc-login-fe" tabindex="-1">OzHera-tpc-login-fe <a class="header-anchor" href="#ozhera-tpc-login-fe" aria-label="Permalink to &quot;OzHera-tpc-login-fe&quot;">​</a></h5><p>ozhera-tpc-login-fe is responsible for building the yaml of the tpclogin login front page.</p><p>Please note:</p><ul><li>The number of pod replicas and pod resource limits should be adjusted based on the amount of traffic.</li></ul><p><img src="'+C+'" alt="ozhera-tpc-login-fe.jpg"></p><h5 id="ozhera-tpc" tabindex="-1">OzHera-tpc <a class="header-anchor" href="#ozhera-tpc" aria-label="Permalink to &quot;OzHera-tpc&quot;">​</a></h5><p>ozhera-tpc is responsible for building the yaml of the tpc service backend.</p><p>Please note:</p><ul><li>The number of pod replicas and pod resource limits should be adjusted based on the amount of traffic;</li><li>The service-related configuration is configured in the nacos configuration above.</li></ul><p><img src="'+j+'" alt="ozhera-tpc.jpg"></p><h5 id="ozhera-tpc-fe" tabindex="-1">OzHera-tpc-fe <a class="header-anchor" href="#ozhera-tpc-fe" aria-label="Permalink to &quot;OzHera-tpc-fe&quot;">​</a></h5><p>ozhera-tpc-fe is responsible for building the yaml of the tpc front page.</p><p>Please note:</p><ul><li>The number of pod replicas and pod resource limits should be adjusted based on the amount of traffic.</li></ul><p><img src="'+I+'" alt="ozhera-tpc-fe.jpg"></p><h5 id="ozhera-app" tabindex="-1">OzHera-app <a class="header-anchor" href="#ozhera-app" aria-label="Permalink to &quot;OzHera-app&quot;">​</a></h5><p>ozhera-app is responsible for the application-related logic operations in the hera system, and can provide various service information for external use through this service.</p><p>Please note:</p><ul><li>The number of pod replicas and pod resource limits should be adjusted based on the amount of traffic.</li></ul><p><img src="'+S+'" alt="ozhera-app.jpg"></p><h5 id="ozhera-log-manager" tabindex="-1">OzHera-log-manager <a class="header-anchor" href="#ozhera-log-manager" aria-label="Permalink to &quot;OzHera-log-manager&quot;">​</a></h5><p>ozhera-log-manager is mainly responsible for the introduction of page application logs and the distribution of various metadata configurations.</p><p>Please note:</p><ul><li>The number of pod replicas and pod resource limits should be adjusted based on the amount of traffic.</li></ul><p><img src="'+E+'" alt="ozhera-log-manager.jpg"></p><h4 id="cluster-deployment" tabindex="-1">Cluster Deployment <a class="header-anchor" href="#cluster-deployment" aria-label="Permalink to &quot;Cluster Deployment&quot;">​</a></h4><ul><li><p>Save Configuration</p><p>After ensuring that step 2.4.4 is completed, click on &quot;Save Configuration&quot;. This step will:</p><ol><li>Retain the entire configuration;</li><li>Replace nacos variables (If there are configurations like ${variable} in the nacos configuration, an automatic round of replacements will be done, with the replacement values derived from the entered connection information and the &quot;access method ip:port&quot; generated in the second step).</li></ol></li><li><p>Activate Cluster</p><p>Once the &quot;Save Configuration&quot; is done, you can click on &quot;Activate Cluster&quot; to deploy the entire hera cluster.</p></li></ul>',115),L=[Q,B,F];function V($,K,Y,G,X,J){return N(),M("div",null,L)}const ee=R(D,[["render",V]]);export{Z as __pageData,ee as default};
